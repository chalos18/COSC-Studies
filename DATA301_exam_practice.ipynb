{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chalos18/COSC-Studies/blob/main/DATA301_exam_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YViVEJ-rV469",
        "outputId": "fcb2e8eb-0a96-4846-db1a-3bb676160976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask\n",
        "from dask import bag as db\n",
        "\n",
        "file1 = db.read_text(\"/content/drive/My Drive/Colab Notebooks/browsing1.txt\")\n",
        "file2 = db.read_text(\"/content/drive/My Drive/Colab Notebooks/browsing2.txt\")\n",
        "\n",
        "print(file1.take(2))\n",
        "print(file2.take(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3MbLFlZUr5N",
        "outputId": "a896fe3b-fdba-4cba-b3a5-09ad7d3f0f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n', 'GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192\\n')\n",
            "('SNA99873 ELE28189 GRO57279 DAI76353 SNA40058 SNA89670 ELE99049 ELE54877\\n', 'DAI83948 DAI56047 GRO78501 FRO14040 GRO31702 SNA82050 SNA85241\\n')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categories(db_bag):\n",
        "  db_bag_map = db_bag.map(lambda line: tuple(sorted(set(word[:3] for word in line.strip().split()))))\n",
        "\n",
        "  return db_bag_map"
      ],
      "metadata": {
        "id": "dhZ7J5ifXSWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_categories(db.from_sequence([\"FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n\"])).take(1)[0]) # result should be ('ELE', 'FRO', 'GRO', 'SNA')\n",
        "print(get_categories(db.from_sequence([\"SNA99873 ELE28189 GRO57279 DAI76353 SNA40058 SNA89670 ELE99049 ELE54877\\n\"])).take(1)[0])\n",
        "print(get_categories(file1).topk(1, lambda s: sorted(s, reverse=True)).compute()[0])\n",
        "print(get_categories(file2).topk(1, lambda s: sorted(s, reverse=True)).compute()[0])\n",
        "print(type(get_categories(db.from_sequence([\"FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n\"])).take(1)[0]) == tuple)\n",
        "print(get_categories(db.from_sequence([\"FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n\"])).take(1)[0] == tuple(['ELE', 'FRO', 'GRO', 'SNA']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADn3f2HupzCs",
        "outputId": "a424c4bf-1897-40df-dab8-77ae87550a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ELE', 'FRO', 'GRO', 'SNA')\n",
            "('DAI', 'ELE', 'GRO', 'SNA')\n",
            "('DAI', 'ELE', 'FRO', 'GRO', 'SNA')\n",
            "('DAI', 'ELE', 'FRO', 'GRO', 'SNA')\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_counts(abag):\n",
        "  frequent_items = abag.frequencies().topk(10, key=1)\n",
        "\n",
        "  return frequent_items\n"
      ],
      "metadata": {
        "id": "oCVy8RyZsnq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_category_counts(db.from_sequence([('ELE', 'FRO', 'GRO', 'SNA'), ('ELE', 'FRO', 'GRO', 'SNA')])).compute())\n",
        "print(get_category_counts(db.from_sequence([('ELE', 'FRO'), ('ELE', 'GRO'), ('ELE', 'FRO')])).compute())\n",
        "print(get_category_counts(get_categories(file1)).compute())\n",
        "print(get_category_counts(get_categories(file2)).compute())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOlzzIu-tDmh",
        "outputId": "d75610e9-b908-4845-c136-639e526459b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('ELE', 'FRO', 'GRO', 'SNA'), 2)]\n",
            "[(('ELE', 'FRO'), 2), (('ELE', 'GRO'), 1)]\n",
            "[(('DAI', 'ELE', 'FRO', 'GRO', 'SNA'), 9828), (('DAI', 'ELE', 'GRO', 'SNA'), 1298), (('DAI', 'ELE', 'FRO', 'GRO'), 1108), (('DAI', 'ELE', 'FRO', 'SNA'), 1057), (('DAI', 'FRO', 'GRO', 'SNA'), 857), (('ELE', 'FRO', 'GRO', 'SNA'), 818), (('DAI', 'FRO', 'SNA'), 222), (('DAI', 'ELE', 'GRO'), 215), (('DAI', 'ELE', 'SNA'), 180), (('DAI', 'FRO', 'GRO'), 175)]\n",
            "[(('DAI', 'ELE', 'FRO', 'GRO', 'SNA'), 8559), (('DAI', 'ELE', 'GRO', 'SNA'), 1138), (('DAI', 'ELE', 'FRO', 'SNA'), 938), (('DAI', 'ELE', 'FRO', 'GRO'), 913), (('ELE', 'FRO', 'GRO', 'SNA'), 694), (('DAI', 'FRO', 'GRO', 'SNA'), 584), (('ELE', 'GRO', 'SNA'), 143), (('DAI', 'ELE', 'GRO'), 142), (('DAI', 'ELE', 'SNA'), 139), (('DAI', 'ELE', 'FRO'), 122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_categories(db_bag):\n",
        "  categories = db_bag.map(lambda line: tuple(sorted(set(word[:3] for word in line.strip().split()))))\n",
        "  return categories"
      ],
      "metadata": {
        "id": "d6yPeryFArhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_categories(db.from_sequence([\"FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n\"])).take(1)[0]) # result should be ('ELE', 'FRO', 'GRO', 'SNA')\n",
        "print(get_categories(db.from_sequence([\"SNA99873 ELE28189 GRO57279 DAI76353 SNA40058 SNA89670 ELE99049 ELE54877\\n\"])).take(1)[0])\n",
        "print(get_categories(file1).topk(1, lambda s: sorted(s, reverse=True)).compute()[0])\n",
        "print(get_categories(file2).topk(1, lambda s: sorted(s, reverse=True)).compute()[0])\n",
        "print(type(get_categories(db.from_sequence([\"FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n\"])).take(1)[0]) == tuple)\n",
        "print(get_categories(db.from_sequence([\"FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\\n\"])).take(1)[0] == tuple(['ELE', 'FRO', 'GRO', 'SNA']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-StRYX1Ghp9u",
        "outputId": "778344ac-7b20-4886-b558-3fbe34c9f12b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('ELE', 'FRO', 'GRO', 'SNA')\n",
            "('DAI', 'ELE', 'GRO', 'SNA')\n",
            "('DAI', 'ELE', 'FRO', 'GRO', 'SNA')\n",
            "('DAI', 'ELE', 'FRO', 'GRO', 'SNA')\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_category_counts(db_bag):\n",
        "  frequent = db_bag.frequencies()\n",
        "  return frequent.topk(10, key=1)"
      ],
      "metadata": {
        "id": "h8R2JIJNBU4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_category_counts(db.from_sequence([('ELE', 'FRO', 'GRO', 'SNA'), ('ELE', 'FRO', 'GRO', 'SNA')])).compute())\n",
        "print(get_category_counts(db.from_sequence([('ELE', 'FRO'), ('ELE', 'GRO'), ('ELE', 'FRO')])).compute())\n",
        "print(get_category_counts(get_categories(file1)).compute())\n",
        "print(get_category_counts(get_categories(file2)).compute())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk7RUc8TBUCN",
        "outputId": "67a8af75-5ec6-4acf-d374-7ca2dc61fdfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('ELE', 'FRO', 'GRO', 'SNA'), 2)]\n",
            "[(('ELE', 'FRO'), 2), (('ELE', 'GRO'), 1)]\n",
            "[(('DAI', 'ELE', 'FRO', 'GRO', 'SNA'), 9828), (('DAI', 'ELE', 'GRO', 'SNA'), 1298), (('DAI', 'ELE', 'FRO', 'GRO'), 1108), (('DAI', 'ELE', 'FRO', 'SNA'), 1057), (('DAI', 'FRO', 'GRO', 'SNA'), 857), (('ELE', 'FRO', 'GRO', 'SNA'), 818), (('DAI', 'FRO', 'SNA'), 222), (('DAI', 'ELE', 'GRO'), 215), (('DAI', 'ELE', 'SNA'), 180), (('DAI', 'FRO', 'GRO'), 175)]\n",
            "[(('DAI', 'ELE', 'FRO', 'GRO', 'SNA'), 8559), (('DAI', 'ELE', 'GRO', 'SNA'), 1138), (('DAI', 'ELE', 'FRO', 'SNA'), 938), (('DAI', 'ELE', 'FRO', 'GRO'), 913), (('ELE', 'FRO', 'GRO', 'SNA'), 694), (('DAI', 'FRO', 'GRO', 'SNA'), 584), (('ELE', 'GRO', 'SNA'), 143), (('DAI', 'ELE', 'GRO'), 142), (('DAI', 'ELE', 'SNA'), 139), (('DAI', 'ELE', 'FRO'), 122)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Dask program using bags that does the following: loads a text file (for example, http://www.gutenberg.org/cache/epub/16328/pg16328.txt) finds and prints the length of the longest word (space delimited) in the file that contains the first letter of your first name and does not contain non-letter characters (i.e. the isalpha() method on a string in python should return True) Full credit for efficient solutions"
      ],
      "metadata": {
        "id": "RMpiYsuCBN25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request # For downloading the dataset\n",
        "\n",
        "filename = 'pg16328.txt'\n",
        "urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/16328/pg16328.txt', filename)\n",
        "print(\"Downloaded:\", filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-0GBCkSBNAN",
        "outputId": "99a9d1a0-3cdd-4679-b7e6-87063a8c9a10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded: pg16328.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textfile = db.read_text('pg16328.txt')\n",
        "\n",
        "textfile.take(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS9tyVFKJUBQ",
        "outputId": "219b80f9-5158-4483-d821-8841e4d48f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('\\ufeffThe Project Gutenberg eBook of Beowulf: An Anglo-Saxon Epic Poem\\n',)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def longest_word(a_bag, name):\n",
        "  filtered = a_bag.map(lambda line: ((word, len(word)) for word in line.strip().split() if word[0] == name[0] and word.isalpha() == True)).flatten().topk(1, key=1)\n",
        "\n",
        "  return filtered\n",
        "\n",
        "result = longest_word(textfile, 'ana')\n",
        "print(result.compute())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrrapSMzInTk",
        "outputId": "d7f921eb-4356-4c07-b3f1-b5027a8c12b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('aforementioned', 14)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.bag as db\n",
        "import re # For regular expressions to clean text\n",
        "\n",
        "# 1. Load the text file\n",
        "# Replace 'your_text_file.txt' with the actual path to your file\n",
        "# For example, using the Gutenberg file from your previous example:\n",
        "filename = 'pg16328.txt'\n",
        "# Assuming you've already downloaded it using urllib.request.urlretrieve\n",
        "# If not, add the download step here:\n",
        "# import urllib.request\n",
        "# urllib.request.urlretrieve('http://www.gutenberg.org/cache/epub/16328/pg16328.txt', filename)\n",
        "\n",
        "text_bag = db.read_text(filename)\n",
        "\n",
        "# Define a function to process each line and generate word shingles\n",
        "def generate_word_shingles(line):\n",
        "    # Convert to lowercase to treat \"The\" and \"the\" as the same word\n",
        "    line = line.lower()\n",
        "    # Remove punctuation and split into words.\n",
        "    # Use re.findall to get all sequences of letters, effectively stripping non-letters.\n",
        "    words = re.findall(r'\\b[a-z]+\\b', line) # Matches contiguous alphabetic characters\n",
        "\n",
        "    shingles = []\n",
        "    # Generate 2-word shingles\n",
        "    for i in range(len(words) - 1):\n",
        "        shingle = (words[i], words[i+1])\n",
        "        shingles.append(shingle)\n",
        "    return shingles\n",
        "\n",
        "# 2. Process each line and generate shingles\n",
        "# Use map to apply the function to each line in the Dask Bag\n",
        "# Then flatten to get a single bag of shingles\n",
        "word_shingles_bag = text_bag.map(generate_word_shingles).flatten()\n",
        "\n",
        "# You can now perform operations on the word_shingles_bag\n",
        "# For example, to see the first 5 shingles:\n",
        "print(\"First 5 word shingles:\")\n",
        "print(word_shingles_bag.take(5))\n",
        "\n",
        "# To count the frequency of each shingle:\n",
        "print(\"\\nTop 10 most frequent word shingles:\")\n",
        "print(word_shingles_bag.frequencies().topk(10, key=1).compute())\n",
        "\n",
        "# If you wanted character shingles of length 2:\n",
        "def generate_char_shingles(line):\n",
        "    line = line.lower()\n",
        "    # You might want to strip non-alphanumeric characters, or keep spaces, depending on your definition\n",
        "    # For simplicity, let's just use all characters for now\n",
        "    chars = list(line.replace(\" \", \"\")) # Remove spaces if you don't want them in shingles\n",
        "\n",
        "    shingles = []\n",
        "    for i in range(len(chars) - 1):\n",
        "        shingle = (chars[i], chars[i+1])\n",
        "        shingles.append(shingle)\n",
        "    return shingles\n",
        "\n",
        "char_shingles_bag = text_bag.map(generate_char_shingles).flatten()\n",
        "\n",
        "print(\"\\nFirst 5 character shingles (excluding spaces):\")\n",
        "print(char_shingles_bag.take(5))"
      ],
      "metadata": {
        "id": "GUmgbCQ2evZt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}